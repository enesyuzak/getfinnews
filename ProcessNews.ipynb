{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news=pd.read_csv(r\"files\\TSLANews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4Y=pd.read_csv(r\"files\\TSLAPrices.csv\")\n",
    "df4Y=df4Y.sort_values(by=[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df_news[df_news[\"Date\"]==\"2016-04-29\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_news[\"Date\"].value_counts()\n",
    "yy=pd.DataFrame(y)\n",
    "yy.reset_index(inplace=True)\n",
    "yy = yy.rename(columns = {'index':'Dates',\"Date\":\"Counts\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt=yy[yy[\"Counts\"]>1]\n",
    "tt[\"Dates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.DataFrame()\n",
    "for i in tt[\"Dates\"]:\n",
    "    temp_df=df_news[df_news[\"Date\"]==i]\n",
    "    s=\"\"\n",
    "    for j,row in temp_df.iterrows():\n",
    "        s+=row[\"Text\"]\n",
    "    dict_news={\"Date\":i,\"Text\":s}\n",
    "    new_df=new_df.append(dict_news,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"Text_Len\"]=new_df[\"Text\"].apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=df_news[df_news[\"Date\"]==\"2020-09-08\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss[\"Len\"]=ss[\"Text\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss[\"Len\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(r\"files\\ConcTSLANews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=pd.merge(df4Y,new_df,on=[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[new_df[\"Date\"]==\"2020-11-03\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"Diff\"]=merged[\"Close\"]-merged[\"Open\"]\n",
    "merged[\"Diff_Perc\"]=merged[\"Diff\"]/merged[\"Open\"]\n",
    "merged.loc[merged['Diff'] < 0, 'Status'] = -1\n",
    "merged.loc[merged['Diff'] > 0, 'Status'] = 1\n",
    "merged.loc[merged['Diff'] == 0, 'Status'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_s[\"DIFF_PERC\"].hist(bins=50)\n",
    "plt.figure(figsize=(20,15))\n",
    "counts, bins, bars = plt.hist(merged[\"Diff_Perc\"],bins=30)\n",
    "print(counts)\n",
    "print(bins)\n",
    "print(bars)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_neg=0.01626928\n",
    "margin_pos=0.02511511\n",
    "\n",
    "mask = (merged['Diff_Perc'] > margin_neg) & (merged['Diff_Perc'] <= margin_pos)\n",
    "merged.loc[mask,\"Diff_Perc\"]=0\n",
    "merged.loc[mask,\"Status\"]=0\n",
    "merged[\"Diff_Perc\"].value_counts(sort=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(r\"files\\merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"wordnet\")\n",
    "#nltk.download(\"stopwords\")\n",
    "en_stop=set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "#Jupyter'in uzun metinleri gösterme özelliğini açıyor:\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(document):\n",
    "    document=re.sub(r\"\\W\",\" \",str(document))\n",
    "    document=re.sub(r\"\\s+[azAz]\\s+\",\" \",document)\n",
    "    document=re.sub(r\"\\^[azAz]\\s+\",\" \",document)\n",
    "    document=re.sub(r\"\\s+\",\" \",document,flags=re.I)\n",
    "    document=re.sub(r\"^b\\s+\",\" \",document)\n",
    "    document=document.lower()\n",
    "    \n",
    "    tokens=document.split()\n",
    "    porter=PorterStemmer()\n",
    "    tokens=[porter.stem(word) for word in tokens]\n",
    "    tokens=[word for word in tokens if word not in en_stop]\n",
    "    tokens=[word for word in tokens if len(word)>3]\n",
    "    \n",
    "    preprocessed_text=\" \".join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,GRU,Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(r\"files\\merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(dataset.columns[dataset.columns.str.contains(\"Unnamed\",case=False)],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.loc[:,[\"Text\",\"Status\"]]\n",
    "dataset[\"Status\"]=dataset[\"Status\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[dataset[\"Status\"]==-1,[\"Status\"]]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8639"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target=dataset[\"Status\"].values.tolist()\n",
    "data=dataset[\"Text\"].values.tolist()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "i:  100\n",
      "i:  200\n",
      "i:  300\n",
      "i:  400\n",
      "i:  500\n",
      "i:  600\n",
      "i:  700\n",
      "i:  800\n",
      "i:  900\n",
      "i:  1000\n",
      "i:  1100\n",
      "i:  1200\n",
      "i:  1300\n",
      "i:  1400\n",
      "i:  1500\n",
      "i:  1600\n",
      "i:  1700\n",
      "i:  1800\n",
      "i:  1900\n",
      "i:  2000\n",
      "i:  2100\n",
      "i:  2200\n",
      "i:  2300\n",
      "i:  2400\n",
      "i:  2500\n",
      "i:  2600\n",
      "i:  2700\n",
      "i:  2800\n",
      "i:  2900\n",
      "i:  3000\n",
      "i:  3100\n",
      "i:  3200\n",
      "i:  3300\n",
      "i:  3400\n",
      "i:  3500\n",
      "i:  3600\n",
      "i:  3700\n",
      "i:  3800\n",
      "i:  3900\n",
      "i:  4000\n",
      "i:  4100\n",
      "i:  4200\n",
      "i:  4300\n",
      "i:  4400\n",
      "i:  4500\n",
      "i:  4600\n",
      "i:  4700\n",
      "i:  4800\n",
      "i:  4900\n",
      "i:  5000\n",
      "i:  5100\n",
      "i:  5200\n",
      "i:  5300\n",
      "i:  5400\n",
      "i:  5500\n",
      "i:  5600\n",
      "i:  5700\n",
      "i:  5800\n",
      "i:  5900\n",
      "i:  6000\n",
      "i:  6100\n",
      "i:  6200\n",
      "i:  6300\n",
      "i:  6400\n",
      "i:  6500\n",
      "i:  6600\n",
      "i:  6700\n",
      "i:  6800\n",
      "i:  6900\n",
      "i:  7000\n",
      "i:  7100\n",
      "i:  7200\n",
      "i:  7300\n",
      "i:  7400\n",
      "i:  7500\n",
      "i:  7600\n",
      "i:  7700\n",
      "i:  7800\n",
      "i:  7900\n",
      "i:  8000\n",
      "i:  8100\n",
      "i:  8200\n",
      "i:  8300\n",
      "i:  8400\n",
      "i:  8500\n",
      "i:  8600\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(data)):\n",
    "    data[i]==process_data(data[i])\n",
    "    if i%100==0: print(\"i: \",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=process_data(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['l']\",\n",
       " \"['a']\",\n",
       " \"['s']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['w']\",\n",
       " \"['e']\",\n",
       " \"['e']\",\n",
       " \"['k']\",\n",
       " \"[',']\",\n",
       " '[]',\n",
       " \"['d']\",\n",
       " \"['e']\",\n",
       " \"['l']\",\n",
       " \"['p']\",\n",
       " \"['h']\",\n",
       " \"['i']\",\n",
       " '[]',\n",
       " \"['(']\",\n",
       " \"['n']\",\n",
       " \"['y']\",\n",
       " \"['s']\",\n",
       " \"['e']\",\n",
       " \"[':']\",\n",
       " '[]',\n",
       " \"['d']\",\n",
       " \"['l']\",\n",
       " \"['p']\",\n",
       " \"['h']\",\n",
       " \"[')']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " \"['d']\",\n",
       " '[]',\n",
       " \"['m']\",\n",
       " \"['o']\",\n",
       " \"['b']\",\n",
       " \"['i']\",\n",
       " \"['l']\",\n",
       " \"['e']\",\n",
       " \"['y']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['(']\",\n",
       " \"['n']\",\n",
       " \"['y']\",\n",
       " \"['s']\",\n",
       " \"['e']\",\n",
       " \"[':']\",\n",
       " '[]',\n",
       " \"['m']\",\n",
       " \"['b']\",\n",
       " \"['l']\",\n",
       " \"['y']\",\n",
       " \"[')']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " \"['n']\",\n",
       " \"['o']\",\n",
       " \"['u']\",\n",
       " \"['n']\",\n",
       " \"['c']\",\n",
       " \"['e']\",\n",
       " \"['d']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " '[]',\n",
       " \"['p']\",\n",
       " \"['a']\",\n",
       " \"['r']\",\n",
       " \"['t']\",\n",
       " \"['n']\",\n",
       " \"['e']\",\n",
       " \"['r']\",\n",
       " \"['s']\",\n",
       " \"['h']\",\n",
       " \"['i']\",\n",
       " \"['p']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['o']\",\n",
       " '[]',\n",
       " \"['p']\",\n",
       " \"['r']\",\n",
       " \"['o']\",\n",
       " \"['d']\",\n",
       " \"['u']\",\n",
       " \"['c']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['e']\",\n",
       " \"['l']\",\n",
       " \"['f']\",\n",
       " \"['-']\",\n",
       " \"['d']\",\n",
       " \"['r']\",\n",
       " \"['i']\",\n",
       " \"['v']\",\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " \"['g']\",\n",
       " '[]',\n",
       " \"['c']\",\n",
       " \"['a']\",\n",
       " \"['r']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['e']\",\n",
       " \"['c']\",\n",
       " \"['h']\",\n",
       " \"['n']\",\n",
       " \"['o']\",\n",
       " \"['l']\",\n",
       " \"['o']\",\n",
       " \"['g']\",\n",
       " \"['y']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['a']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['c']\",\n",
       " \"['o']\",\n",
       " \"['u']\",\n",
       " \"['l']\",\n",
       " \"['d']\",\n",
       " '[]',\n",
       " \"['b']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['v']\",\n",
       " \"['a']\",\n",
       " \"['i']\",\n",
       " \"['l']\",\n",
       " \"['a']\",\n",
       " \"['b']\",\n",
       " \"['l']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['o']\",\n",
       " \"['o']\",\n",
       " \"['n']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['2']\",\n",
       " \"['0']\",\n",
       " \"['1']\",\n",
       " \"['9']\",\n",
       " \"['.']\",\n",
       " '[]',\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['i']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['e']\",\n",
       " \"['p']\",\n",
       " \"['i']\",\n",
       " \"['s']\",\n",
       " \"['o']\",\n",
       " \"['d']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['o']\",\n",
       " \"['f']\",\n",
       " '[]',\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " \"['d']\",\n",
       " \"['u']\",\n",
       " \"['s']\",\n",
       " \"['t']\",\n",
       " \"['r']\",\n",
       " \"['y']\",\n",
       " '[]',\n",
       " \"['f']\",\n",
       " \"['o']\",\n",
       " \"['c']\",\n",
       " \"['u']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"[':']\",\n",
       " '[]',\n",
       " \"['e']\",\n",
       " \"['n']\",\n",
       " \"['e']\",\n",
       " \"['r']\",\n",
       " \"['g']\",\n",
       " \"['y']\",\n",
       " '[]',\n",
       " \"[',']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['e']\",\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " '[]',\n",
       " \"['o']\",\n",
       " '[\"\\'\"]',\n",
       " \"['r']\",\n",
       " \"['e']\",\n",
       " \"['i']\",\n",
       " \"['l']\",\n",
       " \"['l']\",\n",
       " \"['y']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['a']\",\n",
       " \"['l']\",\n",
       " \"['k']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['w']\",\n",
       " \"['i']\",\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['m']\",\n",
       " \"['o']\",\n",
       " \"['t']\",\n",
       " \"['l']\",\n",
       " \"['e']\",\n",
       " \"['y']\",\n",
       " '[]',\n",
       " \"['f']\",\n",
       " \"['o']\",\n",
       " \"['o']\",\n",
       " \"['l']\",\n",
       " '[\"\\'\"]',\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['u']\",\n",
       " \"['t']\",\n",
       " \"['o']\",\n",
       " '[]',\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " \"['d']\",\n",
       " \"['u']\",\n",
       " \"['s']\",\n",
       " \"['t']\",\n",
       " \"['r']\",\n",
       " \"['y']\",\n",
       " '[]',\n",
       " \"['e']\",\n",
       " \"['x']\",\n",
       " \"['p']\",\n",
       " \"['e']\",\n",
       " \"['r']\",\n",
       " \"['t']\",\n",
       " \"[',']\",\n",
       " '[]',\n",
       " \"['j']\",\n",
       " \"['o']\",\n",
       " \"['h']\",\n",
       " \"['n']\",\n",
       " '[]',\n",
       " \"['r']\",\n",
       " \"['o']\",\n",
       " \"['s']\",\n",
       " \"['e']\",\n",
       " \"['v']\",\n",
       " \"['e']\",\n",
       " \"['a']\",\n",
       " \"['r']\",\n",
       " \"[',']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['b']\",\n",
       " \"['o']\",\n",
       " \"['u']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['w']\",\n",
       " \"['h']\",\n",
       " \"['a']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['i']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['c']\",\n",
       " \"['o']\",\n",
       " \"['u']\",\n",
       " \"['l']\",\n",
       " \"['d']\",\n",
       " '[]',\n",
       " \"['m']\",\n",
       " \"['e']\",\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " '[]',\n",
       " \"['f']\",\n",
       " \"['o']\",\n",
       " \"['r']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['e']\",\n",
       " \"['l']\",\n",
       " \"['f']\",\n",
       " \"['-']\",\n",
       " \"['d']\",\n",
       " \"['r']\",\n",
       " \"['i']\",\n",
       " \"['v']\",\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " \"['g']\",\n",
       " '[]',\n",
       " \"['c']\",\n",
       " \"['a']\",\n",
       " \"['r']\",\n",
       " \"['s']\",\n",
       " \"['.']\",\n",
       " \"['l']\",\n",
       " \"['i']\",\n",
       " \"['s']\",\n",
       " \"['t']\",\n",
       " \"['e']\",\n",
       " \"['n']\",\n",
       " '[]',\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['o']\",\n",
       " '[]',\n",
       " \"['f']\",\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " \"['d']\",\n",
       " '[]',\n",
       " \"['o']\",\n",
       " \"['u']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['h']\",\n",
       " \"['o']\",\n",
       " \"['w']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['i']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['w']\",\n",
       " \"['i']\",\n",
       " \"['l']\",\n",
       " \"['l']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['f']\",\n",
       " \"['f']\",\n",
       " \"['e']\",\n",
       " \"['c']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['u']\",\n",
       " \"['t']\",\n",
       " \"['o']\",\n",
       " \"['m']\",\n",
       " \"['a']\",\n",
       " \"['k']\",\n",
       " \"['e']\",\n",
       " \"['r']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['c']\",\n",
       " \"['r']\",\n",
       " \"['o']\",\n",
       " \"['s']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['b']\",\n",
       " \"['o']\",\n",
       " \"['a']\",\n",
       " \"['r']\",\n",
       " \"['d']\",\n",
       " \"[',']\",\n",
       " '[]',\n",
       " \"['w']\",\n",
       " \"['h']\",\n",
       " \"['a']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['d']\",\n",
       " \"['e']\",\n",
       " \"['l']\",\n",
       " \"['p']\",\n",
       " \"['h']\",\n",
       " \"['i']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " \"['d']\",\n",
       " '[]',\n",
       " \"['m']\",\n",
       " \"['o']\",\n",
       " \"['b']\",\n",
       " \"['i']\",\n",
       " \"['l']\",\n",
       " \"['e']\",\n",
       " \"['y']\",\n",
       " \"['e']\",\n",
       " '[\"\\'\"]',\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['p']\",\n",
       " \"['l']\",\n",
       " \"['a']\",\n",
       " \"['y']\",\n",
       " '[]',\n",
       " \"['i']\",\n",
       " \"['s']\",\n",
       " \"[',']\",\n",
       " '[]',\n",
       " \"['h']\",\n",
       " \"['o']\",\n",
       " \"['w']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['p']\",\n",
       " \"['o']\",\n",
       " \"['p']\",\n",
       " \"['u']\",\n",
       " \"['l']\",\n",
       " \"['a']\",\n",
       " \"['r']\",\n",
       " \"['i']\",\n",
       " \"['t']\",\n",
       " \"['y']\",\n",
       " '[]',\n",
       " \"['o']\",\n",
       " \"['f']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['e']\",\n",
       " \"['l']\",\n",
       " \"['f']\",\n",
       " \"['-']\",\n",
       " \"['d']\",\n",
       " \"['r']\",\n",
       " \"['i']\",\n",
       " \"['v']\",\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " \"['g']\",\n",
       " '[]',\n",
       " \"['c']\",\n",
       " \"['a']\",\n",
       " \"['r']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['c']\",\n",
       " \"['o']\",\n",
       " \"['u']\",\n",
       " \"['l']\",\n",
       " \"['d']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['o']\",\n",
       " \"['l']\",\n",
       " \"['v']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['o']\",\n",
       " \"['n']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['o']\",\n",
       " \"['f']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['e']\",\n",
       " \"['i']\",\n",
       " \"['r']\",\n",
       " '[]',\n",
       " \"['b']\",\n",
       " \"['i']\",\n",
       " \"['g']\",\n",
       " \"['g']\",\n",
       " \"['e']\",\n",
       " \"['s']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['i']\",\n",
       " \"['s']\",\n",
       " \"['s']\",\n",
       " \"['u']\",\n",
       " \"['e']\",\n",
       " \"['s']\",\n",
       " \"[',']\",\n",
       " '[]',\n",
       " \"['w']\",\n",
       " \"['h']\",\n",
       " \"['a']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['i']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['w']\",\n",
       " \"['i']\",\n",
       " \"['l']\",\n",
       " \"['l']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['a']\",\n",
       " \"['k']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['f']\",\n",
       " \"['o']\",\n",
       " \"['r']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['m']\",\n",
       " \"['a']\",\n",
       " \"['s']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['m']\",\n",
       " \"['a']\",\n",
       " \"['r']\",\n",
       " \"['k']\",\n",
       " \"['e']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['o']\",\n",
       " '[]',\n",
       " \"['e']\",\n",
       " \"['m']\",\n",
       " \"['b']\",\n",
       " \"['r']\",\n",
       " \"['a']\",\n",
       " \"['c']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['e']\",\n",
       " \"['l']\",\n",
       " \"['f']\",\n",
       " \"['-']\",\n",
       " \"['d']\",\n",
       " \"['r']\",\n",
       " \"['i']\",\n",
       " \"['v']\",\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " \"['g']\",\n",
       " '[]',\n",
       " \"['c']\",\n",
       " \"['a']\",\n",
       " \"['r']\",\n",
       " \"['s']\",\n",
       " \"[',']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['o']\",\n",
       " \"['m']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['d']\",\n",
       " \"['v']\",\n",
       " \"['i']\",\n",
       " \"['c']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['f']\",\n",
       " \"['o']\",\n",
       " \"['r']\",\n",
       " '[]',\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " \"['v']\",\n",
       " \"['e']\",\n",
       " \"['s']\",\n",
       " \"['t']\",\n",
       " \"['o']\",\n",
       " \"['r']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['w']\",\n",
       " \"['h']\",\n",
       " \"['o']\",\n",
       " '[]',\n",
       " \"['w']\",\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['o']\",\n",
       " \"['m']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['e']\",\n",
       " \"['x']\",\n",
       " \"['p']\",\n",
       " \"['o']\",\n",
       " \"['s']\",\n",
       " \"['u']\",\n",
       " \"['r']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['o']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['i']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['p']\",\n",
       " \"['a']\",\n",
       " \"['c']\",\n",
       " \"['e']\",\n",
       " \"[',']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " \"['d']\",\n",
       " '[]',\n",
       " \"['m']\",\n",
       " \"['o']\",\n",
       " \"['r']\",\n",
       " \"['e']\",\n",
       " \"['.']\",\n",
       " \"['a']\",\n",
       " '[]',\n",
       " \"['f']\",\n",
       " \"['u']\",\n",
       " \"['l']\",\n",
       " \"['l']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['r']\",\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " \"['s']\",\n",
       " \"['c']\",\n",
       " \"['r']\",\n",
       " \"['i']\",\n",
       " \"['p']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['f']\",\n",
       " \"['o']\",\n",
       " \"['l']\",\n",
       " \"['l']\",\n",
       " \"['o']\",\n",
       " \"['w']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['v']\",\n",
       " \"['i']\",\n",
       " \"['d']\",\n",
       " \"['e']\",\n",
       " \"['o']\",\n",
       " \"['.']\",\n",
       " \"['a']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['e']\",\n",
       " \"['c']\",\n",
       " \"['r']\",\n",
       " \"['e']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['b']\",\n",
       " \"['i']\",\n",
       " \"['l']\",\n",
       " \"['l']\",\n",
       " \"['i']\",\n",
       " \"['o']\",\n",
       " \"['n']\",\n",
       " \"['-']\",\n",
       " \"['d']\",\n",
       " \"['o']\",\n",
       " \"['l']\",\n",
       " \"['l']\",\n",
       " \"['a']\",\n",
       " \"['r']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['t']\",\n",
       " \"['o']\",\n",
       " \"['c']\",\n",
       " \"['k']\",\n",
       " '[]',\n",
       " \"['o']\",\n",
       " \"['p']\",\n",
       " \"['p']\",\n",
       " \"['o']\",\n",
       " \"['r']\",\n",
       " \"['t']\",\n",
       " \"['u']\",\n",
       " \"['n']\",\n",
       " \"['i']\",\n",
       " \"['t']\",\n",
       " \"['y']\",\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['w']\",\n",
       " \"['o']\",\n",
       " \"['r']\",\n",
       " \"['l']\",\n",
       " \"['d']\",\n",
       " '[\"\\'\"]',\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['b']\",\n",
       " \"['i']\",\n",
       " \"['g']\",\n",
       " \"['g']\",\n",
       " \"['e']\",\n",
       " \"['s']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['e']\",\n",
       " \"['c']\",\n",
       " \"['h']\",\n",
       " '[]',\n",
       " \"['c']\",\n",
       " \"['o']\",\n",
       " \"['m']\",\n",
       " \"['p']\",\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " \"['y']\",\n",
       " '[]',\n",
       " \"['f']\",\n",
       " \"['o']\",\n",
       " \"['r']\",\n",
       " \"['g']\",\n",
       " \"['o']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['o']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['h']\",\n",
       " \"['o']\",\n",
       " \"['w']\",\n",
       " '[]',\n",
       " \"['y']\",\n",
       " \"['o']\",\n",
       " \"['u']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['o']\",\n",
       " \"['m']\",\n",
       " \"['e']\",\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " \"['g']\",\n",
       " \"[',']\",\n",
       " '[]',\n",
       " \"['b']\",\n",
       " \"['u']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " '[]',\n",
       " \"['f']\",\n",
       " \"['e']\",\n",
       " \"['w']\",\n",
       " '[]',\n",
       " \"['w']\",\n",
       " \"['a']\",\n",
       " \"['l']\",\n",
       " \"['l']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['t']\",\n",
       " \"['r']\",\n",
       " \"['e']\",\n",
       " \"['e']\",\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " \"['a']\",\n",
       " \"['l']\",\n",
       " \"['y']\",\n",
       " \"['s']\",\n",
       " \"['t']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " \"['d']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['f']\",\n",
       " \"['o']\",\n",
       " \"['o']\",\n",
       " \"['l']\",\n",
       " '[]',\n",
       " \"['d']\",\n",
       " \"['i']\",\n",
       " \"['d']\",\n",
       " \"['n']\",\n",
       " '[\"\\'\"]',\n",
       " \"['t']\",\n",
       " '[]',\n",
       " \"['m']\",\n",
       " \"['i']\",\n",
       " \"['s']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " '[]',\n",
       " \"['b']\",\n",
       " \"['e']\",\n",
       " \"['a']\",\n",
       " \"['t']\",\n",
       " \"[':']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['e']\",\n",
       " \"['r']\",\n",
       " \"['e']\",\n",
       " '[\"\\'\"]',\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['m']\",\n",
       " \"['a']\",\n",
       " \"['l']\",\n",
       " \"['l']\",\n",
       " '[]',\n",
       " \"['c']\",\n",
       " \"['o']\",\n",
       " \"['m']\",\n",
       " \"['p']\",\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " \"['y']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['a']\",\n",
       " \"['t']\",\n",
       " '[\"\\'\"]',\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['p']\",\n",
       " \"['o']\",\n",
       " \"['w']\",\n",
       " \"['e']\",\n",
       " \"['r']\",\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " \"['g']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['e']\",\n",
       " \"['i']\",\n",
       " \"['r']\",\n",
       " '[]',\n",
       " \"['b']\",\n",
       " \"['r']\",\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " \"['d']\",\n",
       " \"['-']\",\n",
       " \"['n']\",\n",
       " \"['e']\",\n",
       " \"['w']\",\n",
       " '[]',\n",
       " \"['g']\",\n",
       " \"['a']\",\n",
       " \"['d']\",\n",
       " \"['g']\",\n",
       " \"['e']\",\n",
       " \"['t']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " \"['d']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['c']\",\n",
       " \"['o']\",\n",
       " \"['m']\",\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " \"['g']\",\n",
       " '[]',\n",
       " \"['r']\",\n",
       " \"['e']\",\n",
       " \"['v']\",\n",
       " \"['o']\",\n",
       " \"['l']\",\n",
       " \"['u']\",\n",
       " \"['t']\",\n",
       " \"['i']\",\n",
       " \"['o']\",\n",
       " \"['n']\",\n",
       " '[]',\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['e']\",\n",
       " \"['c']\",\n",
       " \"['h']\",\n",
       " \"['n']\",\n",
       " \"['o']\",\n",
       " \"['l']\",\n",
       " \"['o']\",\n",
       " \"['g']\",\n",
       " \"['y']\",\n",
       " \"['.']\",\n",
       " '[]',\n",
       " \"['a']\",\n",
       " \"['n']\",\n",
       " \"['d']\",\n",
       " '[]',\n",
       " \"['w']\",\n",
       " \"['e']\",\n",
       " '[]',\n",
       " \"['t']\",\n",
       " \"['h']\",\n",
       " \"['i']\",\n",
       " \"['n']\",\n",
       " \"['k']\",\n",
       " '[]',\n",
       " \"['i']\",\n",
       " \"['t']\",\n",
       " \"['s']\",\n",
       " '[]',\n",
       " \"['s']\",\n",
       " \"['t']\",\n",
       " \"['o']\",\n",
       " \"['c']\",\n",
       " \"['k']\",\n",
       " '[]',\n",
       " \"['p']\",\n",
       " \"['r']\",\n",
       " \"['i']\",\n",
       " ...]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(document):\n",
    "    tokenized_data=[word_tokenize(d) for d in document]\n",
    "    stop_words=stopwords.words(\"english\")\n",
    "    filtered_words=[]\n",
    "    for word in tokenized_data:\n",
    "        if word not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "    filtered=[str(w).lower() for w in filtered_words]\n",
    "    ps=PorterStemmer()\n",
    "    words=[]\n",
    "    for word in filtered:\n",
    "        words.append(ps.stem(word))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize:\n",
    "tokenized_data=[word_tokenize(d) for d in data[0]]\n",
    "#Stop Words:\n",
    "len(tokenized_data[0])\n",
    "#tokenized_data[0]\n",
    "stop_words=stopwords.words(\"english\")\n",
    "filtered_words=[]\n",
    "for word in tokenized_data:\n",
    "    if word not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "filtered=[str(w).lower() for w in filtered_words]\n",
    "#Stemming: Kelimenin kökünü alır. Kelime sonlarındaki ekler atılır--> Driving=drive\n",
    "ps=PorterStemmer()\n",
    "words=[]\n",
    "for word in filtered:\n",
    "    words.append(ps.stem(word))\n",
    "#Lemmatizing: Kelimenin Kökünü alır. Stemminge göre daha doğru kök alır. \n",
    "#Defaultta kelimelere isim olarak bakılır. Ekstra parametre ile fiil olarakta bakılabilir.--> Driving=Driving\n",
    "lem=WordNetLemmatizer()\n",
    "words=[]\n",
    "for w in filtered:\n",
    "    words.append(lem.lemmatize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her bir haberi temizler\n",
    "def process_text(document):\n",
    "    document=re.sub(r\"\\W\",\" \",str(document))\n",
    "    document=word_tokenize(document)\n",
    "    stop_words=stopwords.words(\"english\")\n",
    "    filtered_words=[]\n",
    "    for word in document:\n",
    "        if word not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "    document=[str(w).lower() for w in filtered_words]\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  500\n",
      "Count:  1000\n",
      "Count:  1500\n",
      "Count:  2000\n",
      "Count:  2500\n",
      "Count:  3000\n",
      "Count:  3500\n",
      "Count:  4000\n",
      "Count:  4500\n",
      "Count:  5000\n",
      "Count:  5500\n",
      "Count:  6000\n",
      "Count:  6500\n",
      "Count:  7000\n",
      "Count:  7500\n",
      "Count:  8000\n",
      "Count:  8500\n"
     ]
    }
   ],
   "source": [
    "data2=[]\n",
    "count=0\n",
    "for d in data:\n",
    "    data2.append(process_text(d))\n",
    "    count+=1\n",
    "    if count%500==0: print(\"Count: \",count)\n",
    "del data\n",
    "data=data2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff=int(len(data)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test=data[:cutoff],data[cutoff:]\n",
    "y_train,y_test=target[:cutoff],target[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kullanıcak max kelime sayısı. Metinlerde en sık geçen 10 bin kelime alınır.\n",
    "num_words=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Her bir kelimenin başına bir sayı geliyor. Liste en çok geçen kelime başa geçecek şekilde hazırlanır.\n",
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens=tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tokens=tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens=[len(tokens) for tokens in x_train_tokens + x_test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens=np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492.28081953929853"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7974"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5514"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1435"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tüm haberlerin boyutlarını eşitlemek için padding verilir.\n",
    "max_tokens=np.mean(num_tokens) + 2* np.std(num_tokens)\n",
    "max_tokens=int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9796272716749623"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verilen paddinge göre datayı kapsama oranı\n",
    "np.sum(num_tokens < max_tokens)/len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#her bir datayı 16798 tane token olacak şekilde tamamlayacağız.\n",
    "x_train_pad=pad_sequences(x_train_tokens,maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pad=pad_sequences(x_test_tokens,maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6911, 1435)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1728, 1435)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7267"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 23,  9,  6])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[25]\n",
    "x_train_pad[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenleri stringe çevirme:\n",
    "idx=tokenizer.word_index\n",
    "#word_index i tersine çevirdik:\n",
    "inverse_map=dict(zip(idx.values(),idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    words=[inverse_map[token] for token in tokens if token != 0]\n",
    "    text=\" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test[25]\n",
    "#tokens_to_string(x_test_tokens[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([528, 112, 765, ...,  23,   9,   6])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=50 # Her kelimeye karşılık 50 uzunluğunda vektör oluşturuluyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(input_dim=num_words,\n",
    "                   output_dim=embedding_size,\n",
    "                   input_length=max_tokens,\n",
    "                   name=\"embedding_layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GRU(units=16, return_sequences=True))\n",
    "model.add(GRU(units=18, return_sequences=True))\n",
    "model.add(GRU(units=4))\n",
    "\n",
    "model.add(Dense(3,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=optimizer,\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 1435, 50)          500000    \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (None, 1435, 16)          3264      \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 1435, 18)          1944      \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 4)                 288       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 505,511\n",
      "Trainable params: 505,511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6911 samples\n",
      "Epoch 1/6\n",
      "6911/6911 [==============================] - 116s 17ms/sample - loss: 0.9436 - accuracy: 0.4778\n",
      "Epoch 2/6\n",
      "6911/6911 [==============================] - 131s 19ms/sample - loss: 0.9434 - accuracy: 0.4778\n",
      "Epoch 3/6\n",
      "6911/6911 [==============================] - 148s 21ms/sample - loss: 0.9433 - accuracy: 0.4778\n",
      "Epoch 4/6\n",
      "6911/6911 [==============================] - 149s 22ms/sample - loss: 0.9432 - accuracy: 0.4778\n",
      "Epoch 5/6\n",
      "6911/6911 [==============================] - 164s 24ms/sample - loss: 0.9432 - accuracy: 0.4778\n",
      "Epoch 6/6\n",
      "6911/6911 [==============================] - 167s 24ms/sample - loss: 0.9432 - accuracy: 0.4778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1943ba8b0c8>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "start_time=datetime.datetime.now()\n",
    "model.fit(x_train_pad,y_train,epochs=10,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:14:36.881964\n"
     ]
    }
   ],
   "source": [
    "end_time=datetime.datetime.now()\n",
    "elapsed=end_time-start_time\n",
    "print(elapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=np.array(y_test)\n",
    "result=model.evaluate(x_test_pad,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9887088261268757, 0.37962964]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
